{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1Hdy9zl0OhRgtfhzvXSzmysoGpKacOLir","authorship_tag":"ABX9TyPAIyTUH2xufuIqbABX1nzi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZS5M9iA5fUw4"},"outputs":[],"source":["import matplotlib.pyplot as plt # plotting library\n","import numpy as np # this module is useful to work with numerical arrays\n","import pandas as pd \n","import random \n","import torch\n","import torchvision\n","from torchvision import transforms\n","from torch.utils.data import DataLoader,random_split\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","source":["#load Dataset\n","def load_dataset():\n","    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n","\n","    train_data_path = '/content/drive/MyDrive/Colab Notebooks/Hela_hw2/train'\n","    valid_data_path = '/content/drive/MyDrive/Colab Notebooks/Hela_hw2/validation'\n","    test_data_path = '/content/drive/MyDrive/Colab Notebooks/Hela_hw2/test'\n","\n","    train_dataset = torchvision.datasets.ImageFolder(root=train_data_path,transform=torchvision.transforms.ToTensor())\n","    test_dataset = torchvision.datasets.ImageFolder(root=test_data_path,transform=torchvision.transforms.ToTensor())\n","    valid_dataset = torchvision.datasets.ImageFolder(root=valid_data_path,transform=torchvision.transforms.ToTensor())\n","\n","    train_iterator = torch.utils.data.DataLoader(train_dataset,batch_size=32,num_workers=0,shuffle=True)\n","    test_iterator = torch.utils.data.DataLoader(test_dataset,batch_size=32,num_workers=0,shuffle=True)\n","    valid_iterator = torch.utils.data.DataLoader(valid_dataset,batch_size=32,num_workers=0,shuffle=True)\n","    \n","    return train_iterator, test_iterator, valid_iterator  "],"metadata":{"id":"aI6NWvhUf6sw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# class VariationalEncoder(nn.Module):\n","#     def __init__(self, latent_dims):  \n","#         super(VariationalEncoder, self).__init__()\n","#         self.conv1 = nn.Conv2d(3, 8, 1, stride=2, padding=1)\n","#         print(\"pass1a\")\n","#         self.conv2 = nn.Conv2d(8, 16, 3, stride=2, padding=1)\n","#         print(\"pass2a\")\n","#         self.batch2 = nn.BatchNorm2d(16)\n","#         print(\"pass2b\")\n","#         self.conv3 = nn.Conv2d(16, 32, 3, stride=2, padding=0)  \n","#         print(\"pass3a\")\n","#         self.linear1 = nn.Linear(3*3*32, 128)\n","#         print(\"pass3b\")\n","#         self.linear2 = nn.Linear(128, latent_dims)\n","#         print(\"pass4a\")\n","#         self.linear3 = nn.Linear(128, latent_dims)\n","#         print(\"pass5\")\n","\n","#         self.N = torch.distributions.Normal(0, 1)\n","#         self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU\n","#         self.N.scale = self.N.scale.cuda()\n","#         self.kl = 0\n","\n","#     def forward(self, x):\n","#         x = x.to(device)\n","#         x = F.relu(self.conv1(x))\n","#         print(\"pass1\")\n","#         x = F.relu(self.batch2(self.conv2(x)))\n","#         print(\"pass2\")\n","#         x = F.relu(self.conv3(x))\n","#         print(\"pass3\")\n","#         x = torch.flatten(x, start_dim=1)\n","#         print(\"pass5\")\n","#         x = F.relu(self.linear1(x))\n","#         print(\"pass6\")\n","#         mu =  self.linear2(x)\n","#         sigma = torch.exp(self.linear3(x))\n","#         z = mu + sigma*self.N.sample(mu.shape)\n","#         self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n","#         return z      "],"metadata":{"id":"QuagXUlWjYu6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# class Decoder(nn.Module):\n","    \n","#     def __init__(self, latent_dims):\n","#         super().__init__()\n","\n","#         self.decoder_lin = nn.Sequential(\n","#             nn.Linear(latent_dims, 128),\n","#             nn.ReLU(True),\n","#             nn.Linear(128, 3 * 3 * 32),\n","#             nn.ReLU(True)\n","#         )\n","\n","#         self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, 3, 3))\n","\n","#         self.decoder_conv = nn.Sequential(\n","#             nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=0),\n","#             nn.BatchNorm2d(16),\n","#             nn.ReLU(True),\n","#             nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n","#             nn.BatchNorm2d(8),\n","#             nn.ReLU(True),\n","#             nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1)\n","#         )\n","        \n","#     def forward(self, x):\n","#         x = self.decoder_lin(x)\n","#         x = self.unflatten(x)\n","#         x = self.decoder_conv(x)\n","#         x = torch.sigmoid(x)\n","#         return x"],"metadata":{"id":"3XXGHnJBkGM4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# class VariationalAutoencoder(nn.Module):\n","#     def __init__(self, latent_dims):\n","#         super(VariationalAutoencoder, self).__init__()\n","#         self.encoder = VariationalEncoder(latent_dims)\n","#         self.decoder = Decoder(latent_dims)\n","\n","#     def forward(self, x):\n","#         x = x.to(device)\n","#         z = self.encoder(x)\n","#         return self.decoder(z)"],"metadata":{"id":"1GlCzqLOkUil"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# class varAuto(nn.Module):\n"," \n","#     def __init__(self, input_dim, z_dim, h_dim):\n","#         super().__init__()\n","#         # encoder\n","#         self.img_2hid = nn.Linear(input_dim, h_dim)\n","\n","#         # one for mu and one for stds, note how we only output\n","#         # diagonal values of covariance matrix. Here we assume\n","#         # the pixels are conditionally independent \n","#         self.hid_2mu = nn.Linear(h_dim, z_dim)\n","#         self.hid_2sigma = nn.Linear(h_dim, z_dim)\n","\n","#         # decoder\n","#         self.z_2hid = nn.Linear(z_dim, h_dim)\n","#         self.hid_2img = nn.Linear(h_dim, input_dim)\n","\n","#     def encode(self, x):\n","#         h = F.relu(self.img_2hid(x))\n","#         mu = self.hid_2mu(h)\n","#         sigma = self.hid_2sigma(h)\n","#         return mu, sigma\n","\n","#     def decode(self, z):\n","#         new_h = F.relu(self.z_2hid(z))\n","#         x = torch.sigmoid(self.hid_2img(new_h))\n","#         return x\n","\n","#     def forward(self, x):\n","#         mu, sigma = self.encode(x)\n","\n","#         # Sample from latent distribution from encoder\n","#         epsilon = torch.randn_like(sigma)\n","#         z_reparametrized = mu + sigma*epsilon\n","\n","#         x = self.decode(z_reparametrized)\n","#         return x, mu, sigma"],"metadata":{"id":"JEKxu8xo4bCq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# torch.manual_seed(0)\n","# import torch\n","# torch.cuda.is_available()\n","# INPUT_DIM = 512\n","# Z_DIM = 20\n","# H_DIM = 200\n","# d = 20\n","# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","# print(f'Selected device: {device}')\n","# vae = VariationalAutoencoder(latent_dims=d)\n","# vae2 = varAuto(512, 20, 200)\n","\n","# lr = 0.01\n","\n","# optim = torch.optim.Adam(vae.parameters(), lr=lr, weight_decay=1e-5)\n","# optim2 = torch.optim.Adam(vae2.parameters(), lr=lr, weight_decay=1e-5)\n","# vae.to(device)\n","# vae2.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FvMODoM4kV7j","executionInfo":{"status":"ok","timestamp":1671003502998,"user_tz":300,"elapsed":440,"user":{"displayName":"Mahtabin Rodela Rozbu","userId":"09336197546949774151"}},"outputId":"c99403e1-2a29-49a9-ff87-9b83213f0e95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Selected device: cuda\n","pass1a\n","pass2a\n","pass2b\n","pass3a\n","pass3b\n","pass4a\n","pass5\n"]},{"output_type":"execute_result","data":{"text/plain":["varAuto(\n","  (img_2hid): Linear(in_features=512, out_features=200, bias=True)\n","  (hid_2mu): Linear(in_features=200, out_features=20, bias=True)\n","  (hid_2sigma): Linear(in_features=200, out_features=20, bias=True)\n","  (z_2hid): Linear(in_features=20, out_features=200, bias=True)\n","  (hid_2img): Linear(in_features=200, out_features=512, bias=True)\n",")"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["# ### Training function\n","# def train_epoch(vae, device, dataloader, optimizer):\n","#     # Set train mode for both the encoder and the decoder\n","#     vae.train()\n","#     train_loss = 0.0\n","#     # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n","#     for x, _ in dataloader: \n","#         # Move tensor to the proper device\n","#         x = x.to(device)\n","#         x_hat, mu, sigma = vae(x)\n","#         # Evaluate loss\n","#         loss = ((x - x_hat)**2).sum()\n","#         kl_div = - torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2))\n","#         loss = loss+kl_div\n","\n","\n","#         # Backward pass\n","#         optimizer.zero_grad()\n","#         loss.backward()\n","#         optimizer.step()\n","#         # Print batch loss\n","#         print('\\t partial train loss (single batch): %f' % (loss.item()))\n","#         train_loss+=loss.item()\n","\n","#     return train_loss / len(dataloader.dataset)"],"metadata":{"id":"tH96asJmp8KU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ### Testing function\n","# def test_epoch(vae, device, dataloader, optim2):\n","#     # Set evaluation mode for encoder and decoder\n","#     vae.eval()\n","#     val_loss = 0.0\n","#     with torch.no_grad(): # No need to track the gradients\n","#         for x, _ in dataloader:\n","#             # Move tensor to the proper device\n","#             x = x.to(device)\n","#             # Encode data\n","#             # encoded_data = vae.encoder(x)\n","#             # Decode data\n","#             x_hat, mu, sigma = vae(x)\n","#             loss = ((x - x_hat)**2).sum()\n","#             kl_div = - torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2))\n","#             loss = loss+kl_div\n","#             val_loss += loss.item()\n","\n","#     return val_loss / len(dataloader.dataset)"],"metadata":{"id":"DD0OFYldqLdz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader, test_loader, valid_loader = load_dataset()"],"metadata":{"id":"bzpheVj1qdCc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# def plot_ae_outputs(encoder,decoder,n=10):\n","#     plt.figure(figsize=(16,4.5))\n","#     targets = test_dataset.targets.numpy()\n","#     t_idx = {i:np.where(targets==i)[0][0] for i in range(n)}\n","#     for i in range(n):\n","#       ax = plt.subplot(2,n,i+1)\n","#       img = test_dataset[t_idx[i]][0].unsqueeze(0).to(device)\n","#       encoder.eval()\n","#       decoder.eval()\n","#       with torch.no_grad():\n","#          rec_img  = decoder(encoder(img))\n","#       plt.imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n","#       ax.get_xaxis().set_visible(False)\n","#       ax.get_yaxis().set_visible(False)  \n","#       if i == n//2:\n","#         ax.set_title('Original images')\n","#       ax = plt.subplot(2, n, i + 1 + n)\n","#       plt.imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')  \n","#       ax.get_xaxis().set_visible(False)\n","#       ax.get_yaxis().set_visible(False)  \n","#       if i == n//2:\n","#          ax.set_title('Reconstructed images')\n","#     plt.show()  "],"metadata":{"id":"MbcXGqMfqvfE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# class vA(nn.Module):\n","#   def __init__(self):\n","#     super(VariationalAutoencoder, self).__init__()\n","#     self.encoder = nn.Sequential(nn.Conv2d(3, 6, kernel_size = 2,stride = 1),\n","#     nn.ReLU(True),nn.Conv2d(6,12, kernel_size =2, stride =1), nn.ReLU(True),\n","#     nn.Conv2d(12, 24, kernel_size = 2, stride =1), nn.ReLU(True)\n","#     )\n","    \n","#     self.decoder = nn.Sequential(nn.ConvTranspose2d(24, 12, kernel_size = 2, stride =1),\n","#     nn.ReLU(True),\n","#     nn.ConvTranspose2d(12, 6, kernel_size = 2, stride =1), nn.ReLU(True),\n","#     nn.ConvTranspose2d(6,3, kernel_size =2, stride = 1), nn.ReLU(True), nn.Tanh()\n","#     )\n","#   def forward(self,x):\n","#     x1 = self.encoder(x)\n","#     x2 = self.decoder(x1)\n","#     return x2, x1"],"metadata":{"id":"Fs2-VfEfmYhu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 20\n","criterion = nn.CrossEntropyLoss()\n","\n","for epoch in range(num_epochs):\n","   train_loss = train_epoch(vae2,device,train_loader,optim2)\n","   val_loss = test_epoch(vae2,device,valid_loader, optim2)\n","   print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n","  #  plot_ae_outputs(vae.encoder,vae.decoder,n=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":548},"id":"55HyKMbEqR8x","executionInfo":{"status":"error","timestamp":1671005899492,"user_tz":300,"elapsed":4027,"user":{"displayName":"Mahtabin Rodela Rozbu","userId":"09336197546949774151"}},"outputId":"625b15b5-0b16-4923-826e-770468dffeac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\t partial train loss (single batch): 5295513.000000\n","\t partial train loss (single batch): 4860259.000000\n","\t partial train loss (single batch): 4560619.000000\n","\t partial train loss (single batch): 4314731.500000\n","\t partial train loss (single batch): 4347328.000000\n","\t partial train loss (single batch): 4147861.000000\n","\t partial train loss (single batch): 3651265.250000\n","\t partial train loss (single batch): 3809716.500000\n","\t partial train loss (single batch): 1876841.250000\n","\n"," EPOCH 1/20 \t train loss 134540.637 \t val loss 110461.220\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-0f64ba28c718>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m    \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m    \u001b[0mplot_ae_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-6b6c310abc63>\u001b[0m in \u001b[0;36mplot_ae_outputs\u001b[0;34m(encoder, decoder, n)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_ae_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mt_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1152x324 with 0 Axes>"]},"metadata":{}}]},{"cell_type":"code","source":["# def show_image(img):\n","#     npimg = img.numpy()\n","#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","\n","\n","# vae.eval()\n","\n","# with torch.no_grad():\n","\n","#     # sample latent vectors from the normal distribution\n","#     latent = torch.randn(128, d, device=device)\n","\n","#     # reconstruct images from the latent vectors\n","#     img_recon = vae.decoder(latent)\n","#     img_recon = img_recon.cpu()\n","\n","#     fig, ax = plt.subplots(figsize=(20, 8.5))\n","#     show_image(torchvision.utils.make_grid(img_recon.data[:100],10,5))\n","#     plt.show()"],"metadata":{"id":"bXgxjjEOumW2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# encoded_samples = []\n","# for sample in tqdm(test_dataset):\n","#     img = sample[0].unsqueeze(0).to(device)\n","#     label = sample[1]\n","#     # Encode image\n","#     vae.eval()\n","#     with torch.no_grad():\n","#         encoded_img  = vae.encoder(img)\n","#     # Append to list\n","#     encoded_img = encoded_img.flatten().cpu().numpy()\n","#     encoded_sample = {f\"Enc. Variable {i}\": enc for i, enc in enumerate(encoded_img)}\n","#     encoded_sample['label'] = label\n","#     encoded_samples.append(encoded_sample)\n","    \n","# encoded_samples = pd.DataFrame(encoded_samples)\n","# encoded_samples\n","\n","\n","# from sklearn.manifold import TSNE\n","# import plotly.express as px\n","\n","# px.scatter(encoded_samples, x='Enc. Variable 0', y='Enc. Variable 1', color=encoded_samples.label.astype(str), opacity=0.7)\n"],"metadata":{"id":"IG2xDBNXuzTR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"v9H9aculukGq"}},{"cell_type":"markdown","source":["The working code"],"metadata":{"id":"LfhhMv8yukS_"}},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"q2\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1Cc70UuyqJN4ZrXe8UFDYow4hO7K5Q4my\n","\"\"\"\n","\n","from IPython.utils.py3compat import encode\n","#Autoencoder\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","\n","import os\n","import random\n","import numpy as np\n","import argparse\n","import sys\n","import time\n","\n","SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","def load_dataset():\n","    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n","\n","    train_data_path = '/content/drive/MyDrive/Colab Notebooks/Hela_hw2/train'\n","    valid_data_path = '/content/drive/MyDrive/Colab Notebooks/Hela_hw2/validation'\n","    test_data_path = '/content/drive/MyDrive/Colab Notebooks/Hela_hw2/test'\n","\n","    train_dataset = torchvision.datasets.ImageFolder(root=train_data_path,transform=torchvision.transforms.ToTensor())\n","    test_dataset = torchvision.datasets.ImageFolder(root=test_data_path,transform=torchvision.transforms.ToTensor())\n","    valid_dataset = torchvision.datasets.ImageFolder(root=valid_data_path,transform=torchvision.transforms.ToTensor())\n","\n","    train_iterator = torch.utils.data.DataLoader(train_dataset,batch_size=32,num_workers=0,shuffle=True)\n","    test_iterator = torch.utils.data.DataLoader(test_dataset,batch_size=32,num_workers=0,shuffle=True)\n","    valid_iterator = torch.utils.data.DataLoader(valid_dataset,batch_size=32,num_workers=0,shuffle=True)\n","    \n","    return train_iterator, test_iterator, valid_iterator      \n","\n","def train(model, device, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    \n","    model.train()\n","    \n","    for (x, y) in iterator:\n","        \n","        x = x.to(device)\n","        # x = x.reshape(-1, 512*382)\n","        optimizer.zero_grad()\n","            \n","        fx, mu, sigma = model(x)\n","        \n","        loss = criterion(fx, x)\n","        kl_div = - torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2))\n","        loss = loss+kl_div\n","        \n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)  \n","\n","def evaluate(model, device, iterator, criterion):\n","    epoch_loss, l = 0, 0\n","    epoch_acc, a = 0, 0\n","    epoch_lossList = []\n","    epoch_accList = []\n","    counter =0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","        for (x, y) in iterator:\n","\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            fx, mu, sigma = model(x)\n","            result = (fx, x)\n","\n","            loss = criterion(fx, x)\n","            kl_div = - torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2))\n","            loss = loss+kl_div\n","            #acc = calculate_accuracy(fx, y)\n","\n","            epoch_loss += loss.item()\n","            #epoch_acc += acc.item()\n","            \n","            l = epoch_loss / len(iterator)\n","            #a = epoch_acc / len(iterator)\n","            counter +-1\n","            if counter %10 ==0:\n","              epoch_lossList.append(l)\n","              #epoch_accList.append(a)\n","            \n","    return l, result, epoch_lossList\n","\n","\n","#not used but was tested\n","class VariationalAutoencoder(nn.Module):\n","  def __init__(self):\n","    super(VariationalAutoencoder, self).__init__()\n","    self.encoder = nn.Sequential(nn.Conv2d(3, 6, kernel_size = 2,stride = 1),\n","    nn.ReLU(True),nn.Conv2d(6,12, kernel_size =2, stride =1), nn.ReLU(True),\n","    nn.Conv2d(12, 24, kernel_size = 2, stride =1), nn.ReLU(True)\n","    )\n","    \n","    self.decoder = nn.Sequential(nn.ConvTranspose2d(24, 12, kernel_size = 2, stride =1),\n","    nn.ReLU(True),\n","    nn.ConvTranspose2d(12, 6, kernel_size = 2, stride =1), nn.ReLU(True),\n","    nn.ConvTranspose2d(6,3, kernel_size =2, stride = 1), nn.ReLU(True), nn.Tanh()\n","    )\n","  def forward(self,x):\n","    x1 = self.encoder(x)\n","    x2 = self.decoder(x1)\n","    return x2, x1\n","\n","#used and tested, that showed promise if the epoch size and network depth were to be increased\n","class VariationalAutoencoder1(nn.Module):\n"," \n","    def __init__(self, input_dim, z_dim, h_dim):\n","        super().__init__()\n","        # encoder\n","        # self.a = nn.Conv2d(3, 6, kernel_size = 2,stride = 1)\n","        # self.b = nn.Conv2d(6,12, kernel_size =2, stride =1)\n","        # self.c = nn.Conv2d(12,24, kernel_size =2, stride =1)\n","        self.img_2hid = nn.Linear(input_dim, h_dim)\n","\n","        # one for mu and one for stds, note how we only output\n","        # diagonal values of covariance matrix. Here we assume\n","        # the pixels are conditionally independent \n","        self.hid_2mu = nn.Linear(h_dim, z_dim)\n","        self.hid_2sigma = nn.Linear(h_dim, z_dim)\n","\n","        # decoder\n","        # self.a2= nn.Sequential(nn.ConvTranspose2d(24, 12, kernel_size = 2, stride =1), nn.ReLU(True))\n","        # self.b2 = nn.Sequential(nn.ConvTranspose2d(12, 6, kernel_size = 2, stride =1), nn.ReLU(True))\n","        # self.c2 = nn.Sequential(nn.ConvTranspose2d(6,3, kernel_size =2, stride = 1))\n","        self.z_2hid = nn.Linear(z_dim, h_dim)\n","        self.hid_2img = nn.Linear(h_dim, input_dim)\n","\n","    def encode(self, x):\n","        h = F.relu(self.img_2hid(x))\n","        mu = self.hid_2mu(h)\n","        sigma = self.hid_2sigma(h)\n","        return mu, sigma\n","\n","    def decode(self, z):\n","        new_h = F.relu(self.z_2hid(z))\n","        x = torch.sigmoid(self.hid_2img(new_h))\n","        return x\n","\n","    def forward(self, x):\n","        mu, sigma = self.encode(x)\n","\n","        # Sample from latent distribution from encoder\n","        epsilon = torch.randn_like(sigma)\n","        z_reparametrized = mu + sigma*epsilon\n","\n","        x = self.decode(z_reparametrized)\n","        return x, mu, sigma\n","\n","train_iterator, test_iterator, valid_iterator= load_dataset()   \n","    \n","                    \n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","\n","model = VariationalAutoencoder1(512, 20, 200).to(device)\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","load_checkpoint, skip_train = False, False\n","\n","#You are recommended to store check points\n","if len(sys.argv) > 1:\n","    if sys.argv[1] == \"c\":\n","        load_checkpoint = True\n","    elif sys.argv[1] == \"s\":\n","        skip_train = True  #Proceed directly to testing\n","\n","\n","### Training ###\n","EPOCHS = 100 #A hint just for people using CPUs to train: 5 epoch (~2min/epoch on cpu) is enough in many cases.\n","SAVE_DIR = 'models'\n","SAVE_PATH = os.path.join(SAVE_DIR, \" var autoencoder.pt\")\n","\n","if not os.path.isdir(SAVE_DIR):\n","    os.makedirs(SAVE_DIR)\n","\n","if load_checkpoint:\n","    checkpoint = torch.load(SAVE_PATH)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    loss = checkpoint['loss']\n","\n","\n","\n","\n","if not skip_train:\n","    time_curr = time.time()\n","    best_val_loss = float('inf')\n","    for epoch in range(EPOCHS):\n","        \n","        train_loss = train(model, device, train_iterator, optimizer, criterion)\n","        valid_loss, vResult, vList = evaluate(model, device, valid_iterator, criterion)\n","\n","        if valid_loss < best_val_loss:\n","            torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                }, SAVE_PATH)\n","            best_val_loss = valid_loss\n","    \n","        print('| Epoch: {0:d} | Train Loss: {1:.4f} | Valid Loss : {2:.4f} | Time: {3:d}'.format(epoch+1, \n","            train_loss, valid_loss, int(time.time() - time_curr)))\n","        time_curr = time.time()\n","    \n","    \n","checkpoint = torch.load(SAVE_PATH)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","epoch = checkpoint['epoch']\n","\n","model.eval()\n","\n","#PLEASE IMPLEMENT YOUR TESTING HERE\n","test_loss, tResult, tList = evaluate(model, device, test_iterator, criterion)\n","print('| Test Loss: {0:.3f} |'.format(test_loss))\n","\n","def imageGeneration(iterator, result):\n","  dict1 = {}\n","  dict2 = {}\n","  fx, x = result\n","  for i, (x,y) in enumerate(iterator):\n","    print(\"Y:\", y)\n","    print(\"x SIZE:\", x.size())\n","\n","    x = x.to(device)\n","    y = y.to(device)\n","    # print(\"shape of y:\",y.size())\n","    # print(\"shape of x:\", x.size())\n","    dict1[y][i]= x\n","    dict2[y][i] = fx\n","\n","  return dict1, dict2\n","\n","### Testing ### \n","fx,x = tResult\n","import matplotlib.pyplot as plt\n","\n","print(fx.size())\n","print(x.size())\n","image = (x[27])\n","image2 = (fx[27])\n","print(plt.imshow(image.T.cpu().data.numpy()))\n","\n","plt.gray()\n","print(plt.imshow(image2.T.cpu().data.numpy(), cmap = 'gray'))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"MxpSoNsrupRH","executionInfo":{"status":"ok","timestamp":1671055594983,"user_tz":300,"elapsed":1251930,"user":{"displayName":"Mahtabin Rodela Rozbu","userId":"09336197546949774151"}},"outputId":"353d7300-46ac-47e7-9320-d270531f6415"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n","| Epoch: 1 | Train Loss: 1862869.4583 | Valid Loss : 536198.5820 | Time: 22\n","| Epoch: 2 | Train Loss: 528911.6441 | Valid Loss : 95857.2012 | Time: 11\n","| Epoch: 3 | Train Loss: 117139.1762 | Valid Loss : 53971.7646 | Time: 13\n","| Epoch: 4 | Train Loss: 78027.1853 | Valid Loss : 40321.1011 | Time: 11\n","| Epoch: 5 | Train Loss: 43029.8435 | Valid Loss : 17221.6711 | Time: 11\n","| Epoch: 6 | Train Loss: 24756.1348 | Valid Loss : 9915.9050 | Time: 11\n","| Epoch: 7 | Train Loss: 14511.5751 | Valid Loss : 7026.9924 | Time: 11\n","| Epoch: 8 | Train Loss: 9177.7824 | Valid Loss : 4460.0487 | Time: 13\n","| Epoch: 9 | Train Loss: 6710.9187 | Valid Loss : 2847.3044 | Time: 11\n","| Epoch: 10 | Train Loss: 4774.5601 | Valid Loss : 2101.5024 | Time: 11\n","| Epoch: 11 | Train Loss: 3574.7516 | Valid Loss : 1660.3319 | Time: 12\n","| Epoch: 12 | Train Loss: 2781.8806 | Valid Loss : 1332.9543 | Time: 11\n","| Epoch: 13 | Train Loss: 2338.5387 | Valid Loss : 1150.9324 | Time: 11\n","| Epoch: 14 | Train Loss: 2024.3977 | Valid Loss : 976.3572 | Time: 11\n","| Epoch: 15 | Train Loss: 1731.0398 | Valid Loss : 867.8988 | Time: 11\n","| Epoch: 16 | Train Loss: 1533.8231 | Valid Loss : 778.4544 | Time: 11\n","| Epoch: 17 | Train Loss: 1423.7283 | Valid Loss : 703.7391 | Time: 12\n","| Epoch: 18 | Train Loss: 1259.0196 | Valid Loss : 662.3441 | Time: 11\n","| Epoch: 19 | Train Loss: 1141.6800 | Valid Loss : 584.7367 | Time: 11\n","| Epoch: 20 | Train Loss: 1035.2757 | Valid Loss : 540.8413 | Time: 11\n","| Epoch: 21 | Train Loss: 957.4363 | Valid Loss : 501.7734 | Time: 11\n","| Epoch: 22 | Train Loss: 887.9185 | Valid Loss : 472.1413 | Time: 11\n","| Epoch: 23 | Train Loss: 836.9253 | Valid Loss : 451.5469 | Time: 11\n","| Epoch: 24 | Train Loss: 796.1190 | Valid Loss : 437.0035 | Time: 11\n","| Epoch: 25 | Train Loss: 743.7977 | Valid Loss : 381.9243 | Time: 11\n","| Epoch: 26 | Train Loss: 684.5940 | Valid Loss : 363.7142 | Time: 11\n","| Epoch: 27 | Train Loss: 640.7381 | Valid Loss : 339.7987 | Time: 11\n","| Epoch: 28 | Train Loss: 602.1692 | Valid Loss : 319.9042 | Time: 11\n","| Epoch: 29 | Train Loss: 567.5766 | Valid Loss : 302.6449 | Time: 11\n","| Epoch: 30 | Train Loss: 537.3166 | Valid Loss : 287.9514 | Time: 11\n","| Epoch: 31 | Train Loss: 508.7862 | Valid Loss : 275.8803 | Time: 12\n","| Epoch: 32 | Train Loss: 494.0173 | Valid Loss : 257.2950 | Time: 11\n","| Epoch: 33 | Train Loss: 463.9627 | Valid Loss : 256.3162 | Time: 11\n","| Epoch: 34 | Train Loss: 439.7130 | Valid Loss : 234.3330 | Time: 11\n","| Epoch: 35 | Train Loss: 424.6177 | Valid Loss : 237.7865 | Time: 11\n","| Epoch: 36 | Train Loss: 410.0801 | Valid Loss : 213.5573 | Time: 11\n","| Epoch: 37 | Train Loss: 380.6138 | Valid Loss : 205.4458 | Time: 11\n","| Epoch: 38 | Train Loss: 361.2582 | Valid Loss : 193.4567 | Time: 12\n","| Epoch: 39 | Train Loss: 342.2135 | Valid Loss : 185.4172 | Time: 11\n","| Epoch: 40 | Train Loss: 326.7531 | Valid Loss : 184.1108 | Time: 11\n","| Epoch: 41 | Train Loss: 322.4985 | Valid Loss : 177.7536 | Time: 11\n","| Epoch: 42 | Train Loss: 309.3503 | Valid Loss : 164.3864 | Time: 11\n","| Epoch: 43 | Train Loss: 292.6453 | Valid Loss : 163.6094 | Time: 11\n","| Epoch: 44 | Train Loss: 308.8845 | Valid Loss : 167.6365 | Time: 12\n","| Epoch: 45 | Train Loss: 295.5775 | Valid Loss : 149.4673 | Time: 12\n","| Epoch: 46 | Train Loss: 267.8910 | Valid Loss : 145.5016 | Time: 11\n","| Epoch: 47 | Train Loss: 253.8490 | Valid Loss : 139.4435 | Time: 11\n","| Epoch: 48 | Train Loss: 244.0337 | Valid Loss : 136.1872 | Time: 11\n","| Epoch: 49 | Train Loss: 240.1925 | Valid Loss : 133.9582 | Time: 11\n","| Epoch: 50 | Train Loss: 234.3603 | Valid Loss : 134.1761 | Time: 11\n","| Epoch: 51 | Train Loss: 230.2657 | Valid Loss : 128.9872 | Time: 11\n","| Epoch: 52 | Train Loss: 237.3434 | Valid Loss : 136.3356 | Time: 11\n","| Epoch: 53 | Train Loss: 226.0475 | Valid Loss : 126.2076 | Time: 11\n","| Epoch: 54 | Train Loss: 225.4422 | Valid Loss : 116.5024 | Time: 11\n","| Epoch: 55 | Train Loss: 209.2822 | Valid Loss : 120.8004 | Time: 11\n","| Epoch: 56 | Train Loss: 207.8704 | Valid Loss : 123.4627 | Time: 11\n","| Epoch: 57 | Train Loss: 209.8527 | Valid Loss : 110.9880 | Time: 11\n","| Epoch: 58 | Train Loss: 198.7459 | Valid Loss : 111.1643 | Time: 12\n","| Epoch: 59 | Train Loss: 192.7260 | Valid Loss : 102.8443 | Time: 11\n","| Epoch: 60 | Train Loss: 186.5346 | Valid Loss : 99.5684 | Time: 11\n","| Epoch: 61 | Train Loss: 181.2095 | Valid Loss : 100.7413 | Time: 11\n","| Epoch: 62 | Train Loss: 173.7347 | Valid Loss : 98.7904 | Time: 11\n","| Epoch: 63 | Train Loss: 175.6350 | Valid Loss : 96.9854 | Time: 11\n","| Epoch: 64 | Train Loss: 187.4811 | Valid Loss : 118.1028 | Time: 11\n","| Epoch: 65 | Train Loss: 195.2339 | Valid Loss : 107.9534 | Time: 11\n","| Epoch: 66 | Train Loss: 178.4331 | Valid Loss : 99.4723 | Time: 11\n","| Epoch: 67 | Train Loss: 159.3710 | Valid Loss : 88.8820 | Time: 11\n","| Epoch: 68 | Train Loss: 165.8640 | Valid Loss : 108.4698 | Time: 11\n","| Epoch: 69 | Train Loss: 185.6788 | Valid Loss : 84.0254 | Time: 11\n","| Epoch: 70 | Train Loss: 160.1204 | Valid Loss : 93.4927 | Time: 11\n","| Epoch: 71 | Train Loss: 154.2380 | Valid Loss : 86.4207 | Time: 11\n","| Epoch: 72 | Train Loss: 143.7067 | Valid Loss : 78.5812 | Time: 12\n","| Epoch: 73 | Train Loss: 153.7340 | Valid Loss : 83.7934 | Time: 11\n","| Epoch: 74 | Train Loss: 168.7215 | Valid Loss : 74.0121 | Time: 11\n","| Epoch: 75 | Train Loss: 130.6525 | Valid Loss : 71.0252 | Time: 11\n","| Epoch: 76 | Train Loss: 126.2793 | Valid Loss : 75.0357 | Time: 11\n","| Epoch: 77 | Train Loss: 126.4776 | Valid Loss : 77.5650 | Time: 11\n","| Epoch: 78 | Train Loss: 126.5866 | Valid Loss : 69.2020 | Time: 11\n","| Epoch: 79 | Train Loss: 120.7361 | Valid Loss : 67.6553 | Time: 11\n","| Epoch: 80 | Train Loss: 115.4927 | Valid Loss : 68.4406 | Time: 11\n","| Epoch: 81 | Train Loss: 114.5936 | Valid Loss : 65.7209 | Time: 11\n","| Epoch: 82 | Train Loss: 114.8421 | Valid Loss : 62.9708 | Time: 11\n","| Epoch: 83 | Train Loss: 128.1757 | Valid Loss : 101.4667 | Time: 11\n","| Epoch: 84 | Train Loss: 188.4210 | Valid Loss : 75.9576 | Time: 12\n","| Epoch: 85 | Train Loss: 141.6657 | Valid Loss : 81.1422 | Time: 11\n","| Epoch: 86 | Train Loss: 122.3943 | Valid Loss : 72.0495 | Time: 13\n","| Epoch: 87 | Train Loss: 113.1656 | Valid Loss : 91.9780 | Time: 11\n","| Epoch: 88 | Train Loss: 144.8562 | Valid Loss : 61.4414 | Time: 11\n","| Epoch: 89 | Train Loss: 123.2304 | Valid Loss : 78.4998 | Time: 11\n","| Epoch: 90 | Train Loss: 134.3797 | Valid Loss : 70.0629 | Time: 11\n","| Epoch: 91 | Train Loss: 112.8527 | Valid Loss : 56.6820 | Time: 11\n","| Epoch: 92 | Train Loss: 98.5812 | Valid Loss : 50.1133 | Time: 11\n","| Epoch: 93 | Train Loss: 94.9480 | Valid Loss : 48.8043 | Time: 11\n","| Epoch: 94 | Train Loss: 92.9889 | Valid Loss : 50.1682 | Time: 12\n","| Epoch: 95 | Train Loss: 98.5838 | Valid Loss : 47.5592 | Time: 11\n","| Epoch: 96 | Train Loss: 87.3784 | Valid Loss : 47.7172 | Time: 11\n","| Epoch: 97 | Train Loss: 85.2916 | Valid Loss : 46.1951 | Time: 11\n","| Epoch: 98 | Train Loss: 81.4683 | Valid Loss : 45.0991 | Time: 11\n","| Epoch: 99 | Train Loss: 82.3282 | Valid Loss : 44.2058 | Time: 12\n","| Epoch: 100 | Train Loss: 79.4240 | Valid Loss : 44.5156 | Time: 14\n","| Test Loss: 67.559 |\n","torch.Size([28, 3, 382, 512])\n","torch.Size([28, 3, 382, 512])\n","AxesImage(54,36;334.8x217.44)\n","AxesImage(54,36;334.8x217.44)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-9aba392eac76>:276: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3277.)\n","  print(plt.imshow(image.T.cpu().data.numpy()))\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["print(fx.size())\n","print(x.size())\n","image = (x[25])\n","image2 = (fx[25])\n","print(plt.imshow(image.T.cpu().data.numpy()))\n","\n","plt.gray()\n","print(plt.imshow(image2.T.cpu().data.numpy(), cmap = 'gray'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":338},"id":"bzNmR2zUhTt6","executionInfo":{"status":"ok","timestamp":1671055719145,"user_tz":300,"elapsed":937,"user":{"displayName":"Mahtabin Rodela Rozbu","userId":"09336197546949774151"}},"outputId":"1e3b7d3e-0a80-4e9b-d3c9-d24943975373"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([28, 3, 382, 512])\n","torch.Size([28, 3, 382, 512])\n","AxesImage(54,36;334.8x217.44)\n","AxesImage(54,36;334.8x217.44)\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}